\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\input{packages}
\input{commands}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{xx}{2018}{xx-xx}{8/18}{xx/xx}{gabava18}{Guillaume Gautier, R\'emi Bardenet, and Michal Valko}

% Short headings should be running head and authors last names

\ShortHeadings{\DPPy}{Gautier, Bardenet, and Valko}
\firstpageno{1}

\begin{document}

\title{\DPPy: A Python toolbox for sampling\\determinantal point processes}

\author{\name Guillaume Gautier$^{*,\dagger}$ \email g.gautier@inria.fr \\
       \name R\'emi Bardenet$^\dagger$ \email remi.bardenet@gmail.com \\
       \name Michal Valko$^*$ \email michal.valko@inria.fr\\
       \addr $^*$SequeL team, INRIA Lille - Nord Europe,  40, avenue Halley 59650, Villeneuve d'Ascq, France\\
       \addr $^\dagger$Univ.\,Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL, 59651 Villeneuve d'Ascq, France
}

\editor{}

\maketitle

\setcounter{footnote}{3}
\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is nontrivial and therefore we present a Python toolbox \DPPy\ that puts together known exact and approximate sampling algorithms.
  The project is hosted on GitHub\footnote{\url{https://github.com/guilgautier/DPPy}} and equipped with an extensive documentation. This documentation\footnote{\label{fn:docs}\url{https://dppy.readthedocs.io}} takes the form of a short survey of DPPs and relates each mathematical property with \DPPy\ objects.
\end{abstract}

\begin{keywords}determinantal point processes, sampling schemes\end{keywords}

\section{Introduction} % (fold)
\label{sec:introduction}

  Determinantal point processes (DPPs) are distributions over configurations of points that encode diversity through a kernel function $K$.
  There were introduced by \citet{Mac75} as models for beams of fermions, and they have since found applications in fields as diverse as probability \citep{Sos00, Kon05, HKPV06}, number theory \citep{RuSa96}, statistical physics \citep{PaBe11}, Monte Carlo methods \citep{BaHa16}, spatial statistics \citep{LaMoRu15}, and machine learning \citep{KuTa12}. In ML, DPPs mainly serve to model diverse sets of items as in recommendation tasks \citep{KaDeKo16, GaPaKo16} or text summarization \citep{DuBa18}.

  The DPPs used in ML so far are finite, that is, they are distributions over subsets of a finite so-called \emph{ground set} of cardinality $M$. In that case, the kernel function becomes a kernel matrix $\bfK$ of size $M\times M$. Routine inference tasks such as normalization, marginalization, or sampling have complexity $\calO(M^3)$ \citep{KuTa12}. This is intractable for large $M$ \citep{Gil14}, as is the case for other kernel machines. In particular, without strong assumptions on $\bfK$, exact sampling requires the eigendecomposition of $\bfK$.
  This prohibitive cost has triggered an effort towards efficient approximate samplers ranging from kernel approximation \citep{AKFT13} to MCMC samplers \citep{AnGhRe16, LiJeSr16c, GaBaVa17}; see also \citep{TrBaAm18} for a survey of sampling algorithms. \DPPy\ provides a turnkey implementation of all general algorithms so far to sample finite DPPs. We also provide a few algorithms for continuous DPPs, although they may not be immediately relevant to ML.

  We hope that packaging together all known sampling algorithms now and in the future will improve reproducibility of papers on DPPs and favour dissemination of results. We also use Travis\footnote{\url{https://travis-ci.com/guilgautier/DPPy}} for continuous integration, both to run tests and certify that \DPPy\ can be used with Linux and MacOS.
  Moreover, \DPPy\ is supported by an extensive documentation\footnoteref{fn:docs}, which provides the essential mathematical background and illustrates some key properties of DPPs through \DPPy\ objects and their methods. \DPPy\ thus also serves as a tutorial.

% section introduction (end)

\section{Determinantal point processes} % (fold)
\label{sec:determinantal_point_processes}

	We give a nontechnical introduction to point processes and DPPs. More details can be found e.g. in \citep{HKPV06}.

  \subsection{Definition} % (fold)
  \label{sub:definition}

    A point process $\calX$ on $\bbX$ is a random subset of points $\lrcb{X_1, \dots, X_N} \subset \bbX$, where the number of points $N$ is itself random. We further add to the definition that $N$ should be almost surely finite and that all points in a sample are distinct. Given a reference measure $\mu$ on $\bbX$, a point process is usually characterized by its $k$-correlation function $\rho_k$ for all $k$, where
    \begin{equation}
    	\Proba{
    		\begin{tabular}{c}
    			$\exists$ one point of the process in\\
    			each ball $B(x_i, \diff x_i), \forall i=1,\dots, k $
    		\end{tabular}
    	}
    	= \rho_k\lrp{x_1,\dots,x_k}
    		\prod_{i=1}^k \mu(\diff x_i),
    \end{equation}
    see \citep[Section 4]{MoWa04}. The functions $\rho_k$ describe the interaction among points in $\calX$ by quantifying cooccurrence of points at a set of locations.
    % Considering $\mu$ as the reference measure, the $k$-correlation functions are defined as
    % \begin{equation}
    % \label{eq:k-correlation_function}
    %   \Expe{ \sum_{
    %     \substack{
    %       (X_1,\dots,X_k) \\
    %       X_1 \neq \dots \neq X_k} }
    %     f(X_1,\dots,X_k)
    %     }
    %     = \int_{\bbX^k}
    %       f(x_1,\dots,x_k) \rho_k(x_1,\dots,x_k)
    %       \prod_{i=1}^k \mu(dx_i),
    % \end{equation}

    % for all bounded measurable functions $f:\bbX^k\to \bbC$ and the sum ranges over $k$-tuples of distinct points of $\calX$.

    A point process $\calX$ on $(\bbX,\mu)$ parametrized by a kernel $K:\bbX\times \bbX\rightarrow \mathbb{C}$ is said to be \emph{determinantal}, denoted as $\calX\sim\DPP(K)$, if its $k$-correlation functions satisfy
	  \begin{equation}
	  \label{eq:k-correlation_function_DPP}
	    \rho_k(x_1,\dots,x_k)
	      = \det \lrb{K(x_i, x_j)}_{i,j=1}^k,
	    \quad \forall k\geq 1.
	  \end{equation}

		\noindent
		Most DPPs in ML correspond to the finite case where $\bbX = \lrcb{1,\dots,M}$ and the reference measure is $\mu=\sum_{i=1}^M \delta_i$ \citep{KuTa12}.
		In this context, the kernel function becomes an $M\times M$ matrix $\bfK$, and the correlation functions refer to inclusion probabilities. DPPs are thus often defined in ML by saying that $\calX\sim \DPP(\bfK)$ if
	  \begin{equation}
	  \label{eq:inclusion_proba_finite}
	    \Proba{S \subset \calX} = \det {\bfK}_S,
	      \quad\forall S\subset \bbX,
	  \end{equation}
		\noindent
    where $\bfK_S$ denotes the submatrix of $\bfK$ formed by the rows and columns indexed by $S$. If we further assume that the kernel matrix $\bfK$ is Hermitian, then existence and unicity of the DPP~\eqref{eq:inclusion_proba_finite} is equivalent to the condition that the spectrum of $\bfK$ should lie in $[0,1]$. The result actually holds for general Hermitian kernel functions $K$ with additional assumptions \cite[Theorem 3]{Sos00}. We note that there exist DPPs with nonsymmetric kernels \citep{BoDiFu10}.

    The main interest of DPPs in ML is that they can model diversity while being tractable. To see where diversity lies, we simply observe that \eqref{eq:inclusion_proba_finite} entails
    $$ \Proba{\{i,j\} \subset \calX} = {\bfK}_{ii}{\bfK}_{jj}-{\bfK}_{ij}{\bfK}_{ji} = \Proba{\{i\} \subset \calX}\Proba{\{i\} \subset \calX} - {\bfK}_{ij}{\bfK}_{ji}.$$
    If $\bfK$ is Hermitian, the quantity ${\bfK}_{ij}{\bfK}_{ji} = \vert{\bfK}_{ij}\vert^2$ thus encodes how less often $i$ and $j$ should cooccur, compared to independent sampling with the same marginals. Most point processes that encode diversity are not tractable, in the sense that we do not have efficient algorithms to sample, marginalize, or compute normalization constants. DPPs are amenable to these tasks \citep{KuTa12}.

  \subsection{Sampling} % (fold)
  \label{sub:sampling}

    \subsubsection{Exact Sampling} % (fold)
    \label{ssub:exact_sampling}

      We assume henceforth that $K$ \todo{XXX}. All DPPs are mixtures of \textit{projection} DPPs, i.e., DPPs parametrized by an orthogonal projection kernel. Namely, given the spectral decomposition of the kernel
      \begin{equation}
      \label{eq:eigdec_kernel}
        K(x,y)\triangleq\suml_{i=1}^{\infty} \lambda_i \phi_i(x) \overline{\phi_i(y)},
        \quad \text{with }
        \int \phi_i(x) \overline{\phi_j(x)} \mu(\diff x) = \delta_{ij},
      \end{equation}
      \citet[Theorem 7]{HKPV06} proved that sampling $\DPP(K)$ can be done by
      \begin{enumerate}
        \item drawing $B_i\sim\Ber(\lambda_i)$ independently, and noting $\lrcb{i_1,\dots,i_{N}} = \lrcb{i:B_i=1}$.
        \item sampling from the \textit{projection} DPP with kernel $\tilde{K}(x,y) = \sum_{n=1}^{N}\phi_{i_n}(x) \overline{\phi_{i_n}(y)}$.
      \end{enumerate}
      The remaining question of sampling from projection DPPs is addressed by \citet[Algorithm 18]{HKPV06}.
      It is based on the chain rule and a lemma that ensures that a \textit{projection} $\DPP(\tilde{K})$ generates configurations of $N=\Tr \tilde{K}$ points almost surely. \todo{I have stopped here so far.}

      To be precise, note that if we denote by $\Phi(x) \triangleq \lrp{\phi_{i_1}(x),\dots,\phi_{i_N}(x)}$ the feature vector associated with $x\in\bbX$, then $\tilde{K}(x,y) = \Phi(y)^{\dagger} \Phi(x)$.
      % $N\overset{\text{a.s.}}{=}\Tr K$ points.
      In particular, the joint distribution of $(X_1,\dots,X_N)$ satisfies
      \begin{equation}
      \label{eq:joint_distribution}
        \frac{1}{N!} \det \lrb{K(x_m,x_n)}_{m,n=1}^N \prod_{n=1}^N\mu(\diff x_n)
          = \frac{1}{N!} \Vol^2\lrcb{\Phi(x_1),\dots,\Phi(x_n)} \prod_{n=1}^N\mu(\diff x_n),
      \end{equation}
			\noindent
      so that the conditional densities appearing in the chain rule take the following form
      \begin{equation}
      \label{eq:conditionals_densities}
        g_1(x)
          = \frac{1}{N} \lrnorm{\phi(x)}^2
          \quad\text{and}\quad
        g_{n | 1:n-1}(x)
          = \frac{1}{N-(n-1)} \lrnorm{ \Pi_{H_{n-1}^{\perp}} \phi(x)}^2\!\!,
      \end{equation}
      % \begin{align*}
      %   g_1(x)           &= \frac{1}{N} \lrnorm{\phi(x)}^2
      %                    &= \frac{1}{N} K(x,x) \\
      %   g_{n | 1:n-1}(x) &= \frac{1}{N-(n-1)}
      %                       \lrnorm{ \Pi_{H_{n-1}^{\perp}} \phi(x)}^2 \\
      %                    &= \frac{1}{N-(n-1)}
      %                       \lrb{K(x,x) - \overline{K(x,x_{1:n-1})} \lrb{K(x_k,x_l)}_{k,l=1}^{n-1} K(x_{1:n-1},x)}
      % \end{align*}
      where $\Pi_{H_{n-1}}$ is the orthogonal projection onto
      $H_{n-1} \triangleq \Span\lrcb{\phi(x_1), \dots, \phi(x_{n-1}) }$.
      The expressions in \Eqref{eq:conditionals_densities} can be expressed as a ratio of two determinants and further expanded with Woodbury's formula.
      Observe that the exact sampling procedure has a strong Gram-Schmidt flavor and the product of the conditional densities in \Eqref{eq:conditionals_densities} refers to the base$\times$height formula.
      In the end, DPPs favors configuration of points whose feature vectors $\Phi(x_1),\dots, \Phi(x_N)$ span a large volume.

      The previous sampling scheme is exact and generic but requires the eigendecomposition of the underlying kernel.
      In the finite setting, this corresponds to an initial $\calO(M^3)$ cost, but then the procedure has average cost of $\calO(M\lrb{\Tr \bfK}^2)$
      % which may still be problematic for large $M$,
      \citep{TrBaAm18}.
      On the other hand, sampling from the conditionals in \Eqref{eq:conditionals_densities} can be cumbersome in the infinite case, since it may require tailored proposal distributions.

      These issues have motivated work on approximate sampling procedures with two main directions: kernel approximation and MCMC samplers.
      It is worth mentioning that some specific DPPs admit special exact samplers, e.g., uniform spanning trees \citep{PrWi98} or eigenvalues of some random matrices \citep{DuEd02}.

    % subsubsection exact_sampling (end)

    \subsubsection{Random Matrices} % (fold)
    \label{ssub:random_matrices}

      Originally, continuous DPPs emerged in quantum mechanics and mathematical physics with the pioneering work of \citet{Wig67}, relating eigenvalues of random matrices to energy levels of heavy atoms.
      That is to say \todo{I'm missing a link}, diagonalizing a properly randomized $N\times N$ matrix is a way to generate exact samples of continuous DPPs at cost $\calO(N^3)$.
      More generally, $\beta$-ensembles for which the case $\beta=2$ refers to a specific class of \textit{projection} DPPs \citep{Kon05} have joint probability distribution
      \begin{equation}
        \frac{1}{Z_{N,\beta}}
        \lrabs{\Delta(x_1,\dots,x_N)}^{\beta}
        \prod_{i= 1}^N
          \mu(d x_i),
      \end{equation}
      where $\lrabs{\Delta(x_1,\dots,x_N)}$ denotes the Vandermonde determinant and $\beta$ plays the role of an inverse temperature controlling the strength of the repulsive interaction between the points.
      The cases $\beta=1, 2,$ and $4,$ together with Gaussian, Gamma, and Beta reference measure~$\mu$ received special attention, since they correspond to the distribution of some random matrix models.
      \citet{DuEd02} unlocked generic $\beta>0$ parameters through equivalent tridiagonal models, thus reducing the sampling cost to $\calO(N^2)$.

    % subsubsection random_matrices (end)

    \subsubsection{Approximate Sampling} % (fold)
    \label{ssub:approximate_sampling}

      Kernel approximation, MCMC, no room

    % subsubsection approximate_sampling (end)

  % subsection sampling (end)

% section determinantal_point_processes (end)

\section{The \DPPy\ toolbox} % (fold)
\label{sec:the_dppy_toolbox}

  \DPPy\ handles objects that fit the natural definition of the different DPPs models.
  \begin{itemize}
	  \item The \DPPy\ object corresponding to the finite $\DPP(\bfK)$ can be instantiated as
	  \begin{nscenter}
	  	\mintinline{python}{Finite_DPP(kernel_type="inclusion", projection=False, **{"K":K})}.
	  \end{nscenter}
		It has two main sampling methods, namely \mintinline{python}{.sample_exact()} and \mintinline{python}{.sample_mcmc()}, implementing different variants of the exact sampling scheme and current state-of-the-art MCMC samplers.

		\item The \DPPy\ object corresponding to $\beta$-ensembles can be instantiated as
		\begin{nscenter}
			\mintinline{python}{Beta_Ensemble(ensemble_name="laguerre", beta=3.14)},
		\end{nscenter}
		when $\mu$ is Gamma (parametrized using the \mintinline{python}{numpy} style).
		It has one sampling method
		\begin{nscenter}
			\mintinline{python}{.sample(sampling_mode="banded", **{"shape":10, "scale":2.0, "size":50})}
		\end{nscenter}
		and two methods for display: \mintinline{python}{.plot()} to plot the last realisation and \mintinline{python}{.hist()} to construct the empirical distribution.
  \end{itemize}

  For more information, please refer to the documentation\footnoteref{fn:docs} and corresponding Jupyter notebooks that showcase the use \DPPy\ objects.

% section the_dppy_toolbox (end)

\section{Conclusion and future work} % (fold)
\label{sec:conclusion_and_future_work}

	The release of \DPPy\ marks the first reproducible and fully documented entry point for using DPPs. It is ready to raise contributors' interest to consolidate the existing content and enlarge the scope of \DPPy\ e.g., with procedures for learning the kernels from data.

% section conclusion_and_future_work (end)

% Acknowledgements should go at the end, before appendices and references
%\newpage
%{\small
\acks{The research presented was supported by European CHIST-ERA project DELTA, French Ministry of
Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universit\"at Magdeburg associated-team north-European project Allocate, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003).}%}

\section{Links to JMLR} % (fold)
\label{sec:links_to_jmlr}
  
  This section is to be removed, it is simply here to link to the JMLR website

  \begin{itemize}
    \item \href{http://www.jmlr.org/mloss/mloss-info.html}{MLOSS website}
    \item \href{http://jmlr.csail.mit.edu/author-info.html}{JMLR author info}
    \item \href{http://www.jmlr.org/format/authors-guide.html}{JMLR author guide}
  \end{itemize}

% section links_to_jmlr (end)


\todo{at the end have nicer and unnified biblio, maybe with michals modified bst for jmlr that treats links nicely}\\
\anstodo{(GG) clickable titles (using miso bst) + arxivId were added. Are we good now?}

\todo{copy $^\dagger$ etc like in ICML paper}\\
\anstodo{(GG) don't get it}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

% \newpage

% \appendix
% \section*{Appendix A.}
% \label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

% \vskip 0.2in

% \bibliographystyle{icml2017}
\bibliography{biblio_new}

\end{document}
